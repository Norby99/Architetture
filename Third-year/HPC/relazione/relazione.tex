\documentclass[a4paper,12pt, oneside]{article}
\title{Progetto di High Performance Computing}
\author{Gabos Norbert 0000970451}
\date{\today}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\pagestyle{plain}
\usepackage{graphicx}
\graphicspath{{images/}}

\begin{document}

\maketitle

\section{Introduzione}

Il programma SPH è un simulatore che si occupa di modellare il comportamento dei fluidi in un
ambiente virtuale. È stato scritto in linguaggio C e progettato per funzionare su un singolo
processore. Per realizzare le versioni parallele del programma, è stato necessario analizzare
la versione sequenziale, al fine di individuare le parti del codice che possono essere eseguite
in parallelo. Le sezioni successive presentano le diverse versioni del programma, con le
relative modifiche e le performance ottenute grazie all'utilizzo di OpenMP e MPI.

\section{Versione OpenMP}

Per creare la versione OpenMP, si è partiti dalla versione sequenziale analizzando il codice.
Una volta individuati i punti da parallelizzare, si sono analizzate eventuali loop carry
dependencies, e si è scoperto che tutti i cicli erano embarrassingly parallel. Si è partiti
dalla funzione più semplice da parallelizzare, ovvero "integrate", in quanto consiste in un
singolo ciclo. Le funzioni compute\_density\_pressure e compute\_forces contengono al loro
interno due cicli annidati, quindi hanno richiesto un po' più di attenzione.
Per la funzione "compute\_density\_pressure", l'intento era quello di parallelizzare entrambi
i cicli mediante la clausola "collapse(2)", però i due cicli sono dipendenti uno dall'altro,
quindi si è deciso di parallelizzare solo il ciclo esterno. Per la funzione "compute\_forces",
l'approccio è stato quello di parallelizzare il ciclo interno, eseguendo una "reduce" sulle
variabili fpress\_x, fpress\_y, fvisc\_x e fvisc\_y. Questa soluzione, purtroppo, è stata
scartata in quanto non ha portato ad un miglioramento delle prestazioni.
Infine, per la funzione "avg\_velocities", si è deciso di utilizzare la funzione reduce sulla
variabile "result" per ottenere il risultato finale.

\subsection{Analisi delle strategie di scheduling}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{scheduling.png}
    \caption{Scheduling}
    \label{fig:scheduling}
\end{figure}

Nell'implementazione OpenMP del programma, sono state valutate diverse strategie di scheduling
per i loop parallelizzati, al fine di determinare l'approccio più efficace per bilanciare il
carico di lavoro tra i thread e massimizzare le prestazioni. In particolare, abbiamo
confrontato il scheduling statico, dinamico con grana fine e dinamico con grana grossa.

\begin{itemize}
\item \textbf{Scheduling statico:} Con questa strategia, i loop vengono divisi in blocchi di
dimensioni uguali e assegnati staticamente ai thread. Il vantaggio di questo approccio è la
riduzione dell'overhead di comunicazione tra i thread, poiché non vi è alcun meccanismo di
load balancing dinamico. Tuttavia, il scheduling statico potrebbe portare a uno squilibrio nel
carico di lavoro, soprattutto se i loop contengono operazioni con tempi di esecuzione variabili.
\item \textbf{Scheduling dinamico con grana fine:} In questa strategia, i loop vengono divisi
in blocchi di dimensioni più piccole rispetto al scheduling statico e assegnati dinamicamente
ai thread disponibili. Il vantaggio di questo approccio è un migliore bilanciamento del carico
di lavoro, a scapito di un maggiore overhead di comunicazione tra i thread.
\item \textbf{Scheduling dinamico con grana grossa:} Questa strategia combina gli aspetti del
scheduling statico e dinamico, dividendo i loop in blocchi di dimensioni intermedie e
assegnandoli dinamicamente ai thread. Ciò consente di ottenere un compromesso tra il bilanciamento
del carico di lavoro e l'overhead di comunicazione.
\end{itemize}

Nella nostra analisi (vedi Figura \ref{fig:scheduling}), si è osservato che il scheduling dinamico con grana grossa ha fornito le
migliori prestazioni complessive, poiché garantisce un buon bilanciamento del carico di lavoro tra
i thread senza introdurre un eccessivo overhead di comunicazione. Tuttavia, è importante notare che
la scelta della strategia di scheduling ottimale dipende dalle specifiche caratteristiche del
problema e dall'architettura hardware utilizzata. Pertanto, si consiglia di eseguire test
empirici per determinare la strategia di scheduling più adatta al caso d'uso specifico.

\section{Versione MPI}

Durante l'inizializzazione della versione MPI, è stata impiegata la funzione Bcast per
distribuire le informazioni relative alle particelle tra i processori. Inoltre, è stato
creato il tipo di variabile "MPI\_PARTICLE" per facilitare l'invio e la ricezione delle
particelle tra i processori.
L'approccio adottato per parallelizzare le varie funzioni ha previsto che ogni processo
avesse le stesse particelle sia all'inizio che alla fine della funzione. A tal fine, è
stata creata la funzione "sync\_particles" che, mediante l'utilizzo di "MPI\_Allgatherv",
permette di raccogliere i dati delle particelle sparse su tutti i processori e di
riorganizzarli in modo che ogni processo abbia le stesse particelle. La funzione
"sync\_particles" viene richiamata alla fine di ogni funzione che modifica le particelle.

Per parallelizzare le funzioni si è diviso l'array in chunk e ogni processo ha eseguito
solamente il proprio chunk. Per quanto riguarda la funzione "avg\_velocities", è stata
utilizzata la funzione "reduce" per ottenere il risultato finale e salvarlo tra tutti i
processi. Questa soluzione avrebbe potuto essere implementata anche attraverso l'uso
della funzione "MPI\_Reduce", ma si è scelto di impiegare "MPI\_Allreduce" per avere 
una soluzione più generale.
Inoltre, alla fine di ogni ciclo, viene richiamata la funzione MPI\_Barrier per garantire
la sincronizzazione tra i processi, anche se questa precauzione potrebbe non essere
strettamente necessaria.

\section{Prestazioni}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{speedup.png}
    \caption{Speedup}
    \label{fig:speedup}
\end{figure}

Nella Figura \ref{fig:speedup}, si può osservare il grafico del speedup ottenuto attraverso
le diverse versioni del programma. È interessante notare come la versione MPI abbia
conseguito un speedup superiore rispetto alla versione OpenMP. Tuttavia, la differenza non
è particolarmente marcata, con un valore medio del 4,71\%.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\textwidth]{strong-scaling.png}
    \caption{Strong Scaling Efficiency}
    \label{fig:strong-scaling}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\textwidth]{weak-scaling.png}
    \caption{Weak Scaling Efficiency}
    \label{fig:weak-scaling}
\end{figure}

Nella Figura \ref{fig:strong-scaling}, è possibile osservare l'andamento dell'efficienza di
strong scaling per entrambe le versioni del programma. Analogamente, la Figura \ref{fig:weak-scaling}
illustra l'efficienza di weak scaling per OpenMP e MPI. L'analisi di queste due figure
permette di avere una visione più completa e dettagliata delle prestazioni ottenute
utilizzando le due diverse implementazioni del programma e di comprendere meglio le loro
caratteristiche in termini di scalabilità. Questo ulteriore confronto conferma l'efficacia
della versione MPI nel garantire una maggiore scalabilità e un migliore utilizzo delle
risorse disponibili rispetto all'implementazione basata su OpenMP.

È importante sottolineare che, nonostante il PC utilizzato disponga di 12 core, le
misurazioni sono state effettuate utilizzando solamente 11 core. Questa scelta è stata
fatta perché, utilizzando tutti i 12 core, le prestazioni del programma peggioravano a
causa di altri processi in esecuzione che richiedevano risorse di sistema.

\section{Conclusioni}

In conclusione, questo lavoro ha presentato l'ottimizzazione del programma SPH attraverso
l'utilizzo di OpenMP e MPI. L'analisi delle prestazioni ha mostrato che la versione MPI
fornisce un miglioramento significativo rispetto alla versione OpenMP, sia in termini di
speedup che di scalabilità. Tuttavia, è importante notare che entrambe le
versioni hanno mostrato miglioramenti rispetto alla versione sequenziale del programma.

Nonostante i miglioramenti ottenuti, esistono altre potenziali ottimizzazioni che potrebbero
essere esplorate in lavori futuri, come l'ottimizzazione delle strutture dati, oppure
l'utilizzo di altre tecnologie, come ad esempio CUDA oppure OpenCL. Inoltre, sarebbe
interessante valutare l'efficacia di queste ottimizzazioni su diverse piattaforme hardware
e sistemi di calcolo ad alte prestazioni.

\end{document}
